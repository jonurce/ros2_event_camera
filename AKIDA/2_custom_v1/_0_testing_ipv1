

import sys
from pathlib import Path
import os
import tensorflow as tf
import onnx

# Disable GPU for TF (quantizeml) -> FORCE quantizeml TO RUN ON CPU
os.environ["CUDA_VISIBLE_DEVICES"] = ""          # ← Disable GPU completely for TF/quantizeml
tf.config.set_visible_devices([], 'GPU')         # ← Extra safety — hide GPU from TF


import quantizeml
import cnn2snn
import akida 
import tf_keras







# ============================================================
# CONFIG 
# ============================================================

# Input / output sizes
W, H = 640, 480              # Original input size (pixels)
W_IN, H_IN = 128, 96         # Model input size (pixels)
W_OUT, H_OUT = 4, 3          # Output heatmap size (out coordinates)


QUANTIZED_FOLDER_PATH = Path("AKIDA/1_ec_example/quantized_models/q8_calib_GOOD_b8_n10")
Q_INT8_PATH = QUANTIZED_FOLDER_PATH / "q_tennst_int8.onnx"
TENNST_PATH = QUANTIZED_FOLDER_PATH / "tennst.onnx"

AKIDA_FOLDER_PATH = QUANTIZED_FOLDER_PATH / "akida_V1_models"
AKIDA_FOLDER_PATH.mkdir(exist_ok=True)

TARGET_VERSION=cnn2snn.AkidaVersion.v1







# ============================================================
# LOAD & VERIFY SAVED QUANTIZED INT8 MODEL (with quantizeml)
# ============================================================

print("\nVerifying saved INT8 model structure...")
with cnn2snn.set_akida_version(TARGET_VERSION):
    model_tennst_onnx = onnx.load(str(TENNST_PATH))
    model_q8_onnx = onnx.load(str(Q_INT8_PATH))

print("---------- TENNST MODEL INFO ----------")
print("Input shape:", [x.dim_value or x.dim_param for x in model_tennst_onnx.graph.input[0].type.tensor_type.shape.dim])
print("Output shape:", [x.dim_value or x.dim_param for x in model_tennst_onnx.graph.output[0].type.tensor_type.shape.dim])

print("---------- INT8 ONNX MODEL INFO ----------")
print("Input shape:", [x.dim_value or x.dim_param for x in model_q8_onnx.graph.input[0].type.tensor_type.shape.dim])
print("Output shape:", [x.dim_value or x.dim_param for x in model_q8_onnx.graph.output[0].type.tensor_type.shape.dim])





# ============================================================
# DEFINE & PRINT ORIGINAL KERAS MODEL
# ============================================================

print("\nDefining original Keras model...")

model_keras = tf_keras.Sequential([
    # Rescaling (standard, maps fine)
    tf_keras.layers.Rescaling(1. / 255, input_shape=(28, 28, 1)),
    
    # First conv (maps to InputConvolutional/Convolutional in v1)
    tf_keras.layers.Conv2D(filters=32, kernel_size=3, strides=2, padding='valid', activation=None),
    tf_keras.layers.ReLU(max_value=6.0),  # Bounded ReLU for conversion
    
    # Separable conv (maps to SeparableConvolutional in v1)
    tf_keras.layers.SeparableConv2D(filters=64, kernel_size=3, strides=2, padding='same', activation=None),
    tf_keras.layers.ReLU(max_value=6.0),
    
    # Final head
    tf_keras.layers.Flatten(),
    tf_keras.layers.Dense(10, activation=None)  # Softmax in loss or post-process
], name='testing_model_v1_compatible')

model_keras.summary()






# ============================================================
# QUANTIZE MODEL 
# ============================================================

print("\nQuantizing...")

qparams = quantizeml.models.QuantizationParams(
    # input_dtype='int8',
    input_weight_bits=8,
    weight_bits=4, # could be 4 for Akida 1.0
    activation_bits=4, # could be 4 for Akida 1.0
)

model_q4 = quantizeml.models.quantize(
    model_keras,
    qparams=qparams,
)

print("Quantization successful!")








# ============================================================
# CHECK COMPATIBILITY FOR AKIDA 2.0
# ============================================================

print("Checking cnn2snn Akida Version")

# current_version = cnn2snn.get_akida_version()
# print(f'Current version: {current_version}')

# cnn2snn.set_akida_version(cnn2snn.AkidaVersion.v1)
# cnn2snn.set_akida_version(version=cnn2snn.AkidaVersion.v1)


with cnn2snn.set_akida_version(TARGET_VERSION):
    updated_version = cnn2snn.get_akida_version()
print(f'Target version: {TARGET_VERSION} \n')
print(f'Updated version: {updated_version} \n')




# print("Checking model compatibility...")

# detected_device = devices()[0]
# virtual_device_v1 = akida.AKD1000()  
# virtual_device_v2 = akida.SixNodesIPv2()

# with cnn2snn.set_akida_version(TARGET_VERSION): 
#     compatibility = cnn2snn.check_model_compatibility(model_keras, device=virtual_device_v1, input_dtype="int8")
    # compatibility = cnn2snn.check_model_compatibility(model_q8_onnx_int8)

# if compatibility['overall'] != 'compatible':
#     print("WARNING: Model has issues. Fix quantization/conversion params and retry.")
#     print(compatibility)  # Shows details
#     exit(1)
# else:
#     print("Model is compatible with Akida 2.0!")

# exit(0)











# ============================================================
# CONVERTION TO AKIDA 2.0 SNN MODEL AND SAVE
# ============================================================

print("Converting to Akida...")
# Convert to Akida SNN model (for Akida 2.0)
try:
    with cnn2snn.set_akida_version(TARGET_VERSION):
        akida_model_q4 = cnn2snn.convert(model_q4)

    print(f"Akida model IP version: {akida_model_q4.ip_version}")
 
    akida_model_q4.summary()

    akida_path = AKIDA_FOLDER_PATH / "akida_v1.fbz"
    akida_model_q4.save(str(akida_path))
    print("Akida SNN model saved → ", akida_path)
    
except Exception as e:
    print(f"Model not fully accelerated by Akida. Reason: {str(e)}")


# ============================================================
# MAP TO AKIDA HARDWARE NSOC
# ============================================================

# devices = akida.devices()
# print(f'Available devices: {[dev.desc for dev in devices]}')
# assert len(devices), "No device found, this example needs an Akida NSoC_v2 device."
# device = devices[0]
# assert device.version == akida.NSoC_v2, "Wrong device found, this example needs an Akida NSoC_v2."
# print(f"Found Akida device: {device}")
# print(f"Akida device IP version: {device.ip_version}")
# print(f"Akida device version: {device.version}\n")

# print(f"Akida HwVersion IP version: {akida.HwVersion.ip_version}\n")

print(f"Akida model device: {akida_model_q4.device}")
print(f"Akida model IP version: {akida_model_q4.ip_version}")
print(f"Akida model MACs: {akida_model_q4.macs}\n")

# print(f"Akida module version: {akida.__version__}")

virtual_device_v1 = akida.AKD1000()  
virtual_device_v2 = akida.SixNodesIPv2()

print(f"Virtual device IP: {virtual_device_v1.ip_version}")  # IpVersion.v2


akida_model_q4.map(virtual_device_v1)  
print("Model mapped!")

# Check model mapping: NP allocation and binary size
akida_model_q4.summary()

