

import sys
from pathlib import Path
import os
import tensorflow as tf
import onnx

# Disable GPU for TF (quantizeml) -> FORCE quantizeml TO RUN ON CPU
os.environ["CUDA_VISIBLE_DEVICES"] = ""          # ← Disable GPU completely for TF/quantizeml
tf.config.set_visible_devices([], 'GPU')         # ← Extra safety — hide GPU from TF


import quantizeml
import cnn2snn
import akida 
import tf_keras







# ============================================================
# CONFIG 
# ============================================================

# Input / output sizes
W, H = 640, 480              # Original input size (pixels)
W_IN, H_IN = 128, 96         # Model input size (pixels)
W_OUT, H_OUT = 4, 3          # Output heatmap size (out coordinates)


QUANTIZED_FOLDER_PATH = Path("AKIDA/1_ec_example/quantized_models/q8_calib_GOOD_b8_n10")
Q_INT8_PATH = QUANTIZED_FOLDER_PATH / "q_tennst_int8.onnx"
TENNST_PATH = QUANTIZED_FOLDER_PATH / "tennst.onnx"

AKIDA_FOLDER_PATH = QUANTIZED_FOLDER_PATH / "akida_V1_models"
AKIDA_FOLDER_PATH.mkdir(exist_ok=True)

TARGET_VERSION=cnn2snn.AkidaVersion.v1







# ============================================================
# LOAD & VERIFY SAVED QUANTIZED INT8 MODEL (with quantizeml)
# ============================================================

print("\nVerifying saved INT8 model structure...")
with cnn2snn.set_akida_version(TARGET_VERSION):
    model_tennst_onnx = onnx.load(str(TENNST_PATH))
    model_q8_onnx = onnx.load(str(Q_INT8_PATH))

print("---------- TENNST MODEL INFO ----------")
print("Input shape:", [x.dim_value or x.dim_param for x in model_tennst_onnx.graph.input[0].type.tensor_type.shape.dim])
print("Output shape:", [x.dim_value or x.dim_param for x in model_tennst_onnx.graph.output[0].type.tensor_type.shape.dim])

print("---------- INT8 ONNX MODEL INFO ----------")
print("Input shape:", [x.dim_value or x.dim_param for x in model_q8_onnx.graph.input[0].type.tensor_type.shape.dim])
print("Output shape:", [x.dim_value or x.dim_param for x in model_q8_onnx.graph.output[0].type.tensor_type.shape.dim])





# ============================================================
# DEFINE & PRINT ORIGINAL KERAS MODEL
# ============================================================

print("\nDefining original Keras model...")

def create_v1_tennst(input_shape=(96, 128, 2)):  # Input: [H=96, W=128, C=2] (no T)
    model = tf_keras.Sequential(name="v1_compatible_tennst")

    # Initial input layer [96, 128, 2]
    model.add(tf_keras.layers.Input(shape=input_shape))  

    # Initial 1x1 conv to expand features Output: [96, 128, 8]
    model.add(tf_keras.layers.Conv2D(filters=8, kernel_size=1, strides=1, padding='same', use_bias=False))
    model.add(tf_keras.layers.ReLU(max_value=6.0))

    # Block 1: output: [48, 64, 16]  (strides=2 halves H and W)
    model.add(tf_keras.layers.SeparableConv2D(filters=16, kernel_size=3, strides=2, padding='same', use_bias=False))
    model.add(tf_keras.layers.ReLU(max_value=6.0))  
    # Output: [24, 32, 16]  (pooling halves again)
    model.add(tf_keras.layers.MaxPooling2D(pool_size=2, padding='same'))
    

    # Block 2: output: [12, 16, 32]
    model.add(tf_keras.layers.SeparableConv2D(filters=32, kernel_size=3, strides=2, padding='same', use_bias=False))
    model.add(tf_keras.layers.ReLU(max_value=6.0))
    # Output: [6, 8, 32]
    model.add(tf_keras.layers.MaxPooling2D(pool_size=2, padding='same'))

    # Block 3: output: [3, 4, 48]
    model.add(tf_keras.layers.SeparableConv2D(filters=48, kernel_size=3, strides=2, padding='same', use_bias=False))
    model.add(tf_keras.layers.ReLU(max_value=6.0)) 
    # Output: [2, 2, 48]  (with padding='same', it may round up/down differently)
    model.add(tf_keras.layers.MaxPooling2D(pool_size=2, padding='same'))

    # Block 4: Output: [1, 1, 80]  (strides=2 on 2x2 → 1x1)
    model.add(tf_keras.layers.SeparableConv2D(filters=80, kernel_size=3, strides=2, padding='same', use_bias=False))
    model.add(tf_keras.layers.ReLU(max_value=6.0))

    # Block 5 (depthwise-like): Output: [1, 1, 112]  (strides=2 on 1x1 remains 1x1)
    model.add(tf_keras.layers.SeparableConv2D(filters=112, kernel_size=3, strides=2, padding='same', use_bias=False))
    model.add(tf_keras.layers.ReLU(max_value=6.0)) 

    # Final head: output: [1, 1, 256]
    model.add(tf_keras.layers.Conv2D(filters=256, kernel_size=1, use_bias=False))
    model.add(tf_keras.layers.ReLU(max_value=6.0)) 
    # Final output: [1, 1, 3]  (confidence, x-offset, y-offset)
    model.add(tf_keras.layers.Conv2D(filters=3, kernel_size=1, activation=None))
    

    return model

# Test
model_keras = create_v1_tennst()
model_keras.summary()






# ============================================================
# QUANTIZE MODEL 
# ============================================================

print("\nQuantizing...")

qparams = quantizeml.models.QuantizationParams(
    input_dtype='int8',
    input_weight_bits=4,
    weight_bits=4, # could be 4 for Akida 1.0
    activation_bits=4, # could be 4 for Akida 1.0
)

model_q4 = quantizeml.models.quantize(
    model_keras,
    qparams=qparams,
)

print("Quantization successful!")








# ============================================================
# CHECK COMPATIBILITY FOR AKIDA 2.0
# ============================================================

print("Checking cnn2snn Akida Version")

# current_version = cnn2snn.get_akida_version()
# print(f'Current version: {current_version}')

# cnn2snn.set_akida_version(cnn2snn.AkidaVersion.v1)
# cnn2snn.set_akida_version(version=cnn2snn.AkidaVersion.v1)


with cnn2snn.set_akida_version(TARGET_VERSION):
    updated_version = cnn2snn.get_akida_version()
print(f'Target version: {TARGET_VERSION} \n')
print(f'Updated version: {updated_version} \n')




# print("Checking model compatibility...")

# detected_device = devices()[0]
# virtual_device_v1 = akida.AKD1000()  
# virtual_device_v2 = akida.SixNodesIPv2()

# with cnn2snn.set_akida_version(TARGET_VERSION): 
#     compatibility = cnn2snn.check_model_compatibility(model_keras, device=virtual_device_v1, input_dtype="int8")
    # compatibility = cnn2snn.check_model_compatibility(model_q8_onnx_int8)

# if compatibility['overall'] != 'compatible':
#     print("WARNING: Model has issues. Fix quantization/conversion params and retry.")
#     print(compatibility)  # Shows details
#     exit(1)
# else:
#     print("Model is compatible with Akida 2.0!")

# exit(0)











# ============================================================
# CONVERTION TO AKIDA 2.0 SNN MODEL AND SAVE
# ============================================================

print("Converting to Akida...")
# Convert to Akida SNN model (for Akida 2.0)
try:
    with cnn2snn.set_akida_version(TARGET_VERSION):
        akida_model_q4 = cnn2snn.convert(model_q4)

    print(f"Akida model IP version: {akida_model_q4.ip_version}")
 
    akida_model_q4.summary()

    akida_path = AKIDA_FOLDER_PATH / "akida_v1_tennst.fbz"
    akida_model_q4.save(str(akida_path))
    print("Akida SNN model saved → ", akida_path)
    
except Exception as e:
    print(f"Model not fully accelerated by Akida. Reason: {str(e)}")


# ============================================================
# MAP TO AKIDA HARDWARE NSOC
# ============================================================

# devices = akida.devices()
# print(f'Available devices: {[dev.desc for dev in devices]}')
# assert len(devices), "No device found, this example needs an Akida NSoC_v2 device."
# device = devices[0]
# assert device.version == akida.NSoC_v2, "Wrong device found, this example needs an Akida NSoC_v2."
# print(f"Found Akida device: {device}")
# print(f"Akida device IP version: {device.ip_version}")
# print(f"Akida device version: {device.version}\n")

# print(f"Akida HwVersion IP version: {akida.HwVersion.ip_version}\n")

print(f"Akida model device: {akida_model_q4.device}")
print(f"Akida model IP version: {akida_model_q4.ip_version}")
print(f"Akida model MACs: {akida_model_q4.macs}\n")

# print(f"Akida module version: {akida.__version__}")

virtual_device_v1 = akida.AKD1000()  
virtual_device_v2 = akida.SixNodesIPv2()

print(f"Virtual device IP: {virtual_device_v1.ip_version}")  # IpVersion.v2


akida_model_q4.map(virtual_device_v1)  
print("Model mapped!")

# Check model mapping: NP allocation and binary size
akida_model_q4.summary()

